{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfcf5e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:2px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:10pt;}\n",
       "div.text_cell_render.rendered_html{font-size:10pt;}\n",
       "div.output {font-size:10pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:10pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:11pt;padding:4px;}\n",
       "table.dataframe{font-size:10px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:2px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:10pt;}\n",
    "div.text_cell_render.rendered_html{font-size:10pt;}\n",
    "div.output {font-size:10pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:10pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:11pt;padding:4px;}\n",
    "table.dataframe{font-size:10px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03b1e5",
   "metadata": {},
   "source": [
    "# <span style='color:red'>ch2. LLM활용의 기본 개념(Ollama)</span>\n",
    "\n",
    "# 1. LLM을 활용하여 답변 생성하기\n",
    "\n",
    "## 1) Ollama 이용한 로컬 LLM 이용\n",
    "- 성능은 GPT, Claude 같은 모델보다 떨어지나, 개념 설명을 위해 open source 모델 사용\n",
    "\n",
    "### ⓐ ollama.com 다운로드 -> 설치 -> 모델 pull\n",
    "- ollama pull deepseek-r1:1.5b (window키+R => powershell창)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64ed974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\n한국의面积约 6,943,800㎡', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-06-25T02:11:42.4753079Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1391983200, 'load_duration': 18995500, 'prompt_eval_count': 12, 'prompt_eval_duration': 300080500, 'eval_count': 20, 'eval_duration': 1072255200, 'model_name': 'deepseek-r1:1.5b'}, id='run--cc4093f5-1711-4875-8f36-784f246bcddf-0', usage_metadata={'input_tokens': 12, 'output_tokens': 20, 'total_tokens': 32})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model='deepseek-r1:1.5b')\n",
    "result = llm.invoke('What is the capital of Korea?')\n",
    "result  # 추론 모델<think>~<think>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01ce1b",
   "metadata": {},
   "source": [
    "### ⓑ ollama.com 다운로드 -> 설치 -> 모델 pull\n",
    "- ollama run llama3.2:1b (window키+R => powershell창)\n",
    "- llama : 공식적으로 한글지원 안 됨 (llama 3.1 405b 한글지원 가능 -> llama 3.3 70b)\n",
    "- exaone : 공식적으로 한글지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a224824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of South Korea is Seoul. The government and many institutions are located in Seoul, and it serves as a major cultural and economic hub for the country. However, the country also has several other cities that serve as provincial capitals, including Daejeon, Busan, and Jeju Island.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T02:19:01.2495091Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3712257100, 'load_duration': 21594200, 'prompt_eval_count': 32, 'prompt_eval_duration': 59808000, 'eval_count': 62, 'eval_duration': 3630224900, 'model_name': 'llama3.2:1b'}, id='run--25aee552-6540-4f33-924f-7752c0a8a624-0', usage_metadata={'input_tokens': 32, 'output_tokens': 62, 'total_tokens': 94})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "result = llm.invoke('What is the capital of Korea?')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab74f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of South Korea is Seoul. The government and many institutions are located in Seoul, and it serves as a major cultural and economic hub for the country. However, the country also has several other cities that serve as provincial capitals, including Daejeon, Busan, and Jeju Island.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9aefea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 Seoul로, 이는 한국의 주도城市이기도 하다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke('한국의 수도는 어디야?')\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a1de7",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "- pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71530c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the capital of Korea?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:690\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[0;32m    684\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[0;32m    685\u001b[0m         )\n\u001b[0;32m    686\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client\n\u001b[0;32m    688\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_httpx_client(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_api_base, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_timeout)\n\u001b[0;32m    689\u001b[0m     }\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\openai\\_client.py:126\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    124\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "result = llm.invoke('What is the capital of Korea?')\n",
    "result  # 에러 이유 : OPENAI_API_KEY 환경 변수 부재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beddd57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 가져오기\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 OPENAI_API_KEY 읽어오기(.env 못 씀)\n",
    "# 보안키 추가 후\n",
    "# from google.colab import userdata\n",
    "# userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3458f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='한국의 수도는 서울입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 19, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BmBeiiNlr3oHC4VAxXuAcS0ILJMK6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a2bd90f1-0071-4e0b-8fc9-6da46e50490a-0', usage_metadata={'input_tokens': 19, 'output_tokens': 7, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', \n",
    "                 # openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                )\n",
    "llm.invoke('What is the capital of Korea? Answer me in koeran')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델의 키가 OPENAI_API_KEY는 아님\n",
    "# Claude -> Anthropic\n",
    "# Azure, upstage, Bedrock : 에러를 메세지 참조하여 환경변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72adf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureOpenAI\n",
    "# llm = AzureOpenAI(model='gpt-4o-mini')\n",
    "# 에러를 내면 OPENAI_API_VERSION 환경변수가 필요하다는 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d7f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620')\n",
    "# llm.invoke('What is the capital of Korea?')\n",
    "# 에러 메세지를 봐도 환경변수 이름을 알 수 없음 -> ChatAnthropic 검색 후 \n",
    "# langchain dose 에서 명시한 ANTHROPIC_API_KEY 이름의 환경병수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404a0a5",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성하기\n",
    "- 프롬프트 : LLM호출시 쓰는 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5daac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "# llm.invoke(0)\n",
    "# 프롬프트 타입 : 스트링, PromptValue, BaseMessage 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f37a87",
   "metadata": {},
   "source": [
    "## 1) 기본 프롬프트 템플릿 사용\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "469ab7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of korea'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Korea is Seoul.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T05:32:01.0877917Z', 'done': True, 'done_reason': 'stop', 'total_duration': 492643900, 'load_duration': 20018900, 'prompt_eval_count': 32, 'prompt_eval_duration': 58091100, 'eval_count': 8, 'eval_duration': 414533900, 'model_name': 'llama3.2:1b'}, id='run--ce047836-d683-4351-897d-af5af92e273d-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "prompt_template = PromptTemplate(\n",
    "    template = 'What is the capital of {country}',  # {} 안의 값을 새로운 값으로 할당 가능\n",
    "    input_variables = ['country']\n",
    ")\n",
    "prompt = prompt_template.invoke({'country':'korea'})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59c325",
   "metadata": {},
   "source": [
    "## 2) 메세지 기반 프롬프트 작성\n",
    "- BaseMessage 리스트\n",
    "- BaseMessage 상속 받은 클래스 : AIMessage, HummanMessage, SystmMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cae5575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T05:52:11.0483614Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1003059900, 'load_duration': 20033700, 'prompt_eval_count': 85, 'prompt_eval_duration': 551856400, 'eval_count': 8, 'eval_duration': 428718400, 'model_name': 'llama3.2:1b'}, id='run--9b1fc687-0005-4709-897b-a36649680019-0', usage_metadata={'input_tokens': 85, 'output_tokens': 8, 'total_tokens': 93})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "message_list = [\n",
    "    SystemMessage(content='You ara a helpful assistant!'),\n",
    "    HumanMessage(content='What is the capital of korea?'),\n",
    "    AIMessage(content='The capital of Korea is Seoul'),\n",
    "    HumanMessage(content='What is the capital of Italy?'),\n",
    "    AIMessage(content='The capital of Italy is Rome'),\n",
    "    HumanMessage(content='What is the capital of France?'),\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27d863a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You ara a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of korea?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Korea is Seoul', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of {country}?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# BaseMessage list로 하면로 렝체인화 X, ChatPromptTemplate X\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "message_list = [\n",
    "    SystemMessage(content='You ara a helpful assistant!'),\n",
    "    HumanMessage(content='What is the capital of korea?'),\n",
    "    AIMessage(content='The capital of Korea is Seoul'),\n",
    "    HumanMessage(content='What is the capital of Italy?'),\n",
    "    AIMessage(content='The capital of Italy is Rome'),\n",
    "    HumanMessage(content='What is the capital of {country}?'),\n",
    "]\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(message_list)\n",
    "prompt = chatPromptTemplate.invoke({'country':'korea'})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0706ad",
   "metadata": {},
   "source": [
    "## 3) ChatPromptTemplate 사용\n",
    "- BaseMessage 리스트 -> 튜플 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b9f8c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금한가요?필리핀\n",
      "프롬프트 :  messages=[SystemMessage(content='You ara a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of 필리핀?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of the Philippines is Manila.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 BaseMessage를 수정\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", 'You ara a helpful assistant!'),\n",
    "    ('human', 'What is the capital of {country}?'),\n",
    "])\n",
    "country = input('어느 나라 수도가 궁금한가요?')\n",
    "prompt = chatPromptTemplate.invoke({'country':country})\n",
    "print('프롬프트 : ', prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a1b3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금한가요?뉴질랜드\n",
      "프롬프트 :  messages=[SystemMessage(content='넌 훌륭한 지리학 박사야', additional_kwargs={}, response_metadata={}), HumanMessage(content='뉴질랜드의 수도가 어디인가요?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ニュ Zealand의 수도는 Wellington입니다.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 BaseMessage를 수정\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '넌 훌륭한 지리학 박사야'),\n",
    "    ('human', '{country}의 수도가 어디인가요?'),\n",
    "])\n",
    "country = input('어느 나라 수도가 궁금한가요?')\n",
    "prompt = chatPromptTemplate.invoke({'country':country})\n",
    "print('프롬프트 : ', prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d426aab",
   "metadata": {},
   "source": [
    "# 3. 답변 형식을 컨트롤 하기\n",
    "- invoke 실행결과는 AIMessage() -> String 이나 json, 객체 : outputParser이용\n",
    "\n",
    "## 1) 문자열 출력 퍼서 이용\n",
    "- StroutputParser를 사용하여 LLM출력(AIMessage)을 단순 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cf6e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트 :  text='What is the capital if korea. Return the name of the city only'\n",
      "LLM 결과 :  <class 'langchain_core.messages.ai.AIMessage'> content='Seoul' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T06:39:35.1755325Z', 'done': True, 'done_reason': 'stop', 'total_duration': 207408300, 'load_duration': 25080700, 'prompt_eval_count': 40, 'prompt_eval_duration': 63025800, 'eval_count': 3, 'eval_duration': 117408100, 'model_name': 'llama3.2:1b'} id='run--043ee2d5-f063-45d2-baf8-a68be0b2a49f-0' usage_metadata={'input_tokens': 40, 'output_tokens': 3, 'total_tokens': 43}\n",
      "파서 결과 :  Seoul\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = 'What is the capital if {country}. Return the name of the city only',\n",
    "    input_variables = ['country']\n",
    ")\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({'country':'korea'})\n",
    "print('프롬프트 : ', prompt)\n",
    "result = llm.invoke(prompt)\n",
    "print('LLM 결과 : ', type(result), result)\n",
    "# 문자열 출력 파서를 이용하여 LLM응갑을 단순 문자열 변환\n",
    "output_parser = StrOutputParser()\n",
    "print('파서 결과 : ', output_parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "693b76dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(prompt_template.invoke({'country':'korea'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2916a002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Korea is Seoul.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_template(변수설정) -> chatPromptTemplate(변수설정, system과 모범답안 지정)\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful assistant with espertise in South korea'),\n",
    "    ('human', 'What is the capital of {country} Return the name if the city only.')\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({'country':'korea'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752eb5d7",
   "metadata": {},
   "source": [
    "##  2) 응답 타입 확인\n",
    "- json()으로 응답하기 원하지만, 우선 어떤 형식으로 변환되는 확인\n",
    "- {'name':'홍', 'age':'22}(json) / {'name':'홍', 'age':22}(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "416a2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'> text='Give following information about Korea\\n    - Capital\\n    - Population\\n    - Language\\n    - Currency\\n    return it is JSON format and return the JSON dictionary only'\n",
      "<class 'langchain_core.prompt_values.StringPromptValue'> content='```\\n{\\n    \"capital\": \"Seoul\",\\n    \"population\": 51000000,\\n    \"language\": \"Korean\",\\n    \"currency\": \"South Korean won\"\\n}\\n```' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T07:04:35.2476051Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2988926500, 'load_duration': 28350800, 'prompt_eval_count': 59, 'prompt_eval_duration': 635833600, 'eval_count': 40, 'eval_duration': 2324742100, 'model_name': 'llama3.2:1b'} id='run--6191386a-4cfa-47c8-9a50-5793b178fb16-0' usage_metadata={'input_tokens': 59, 'output_tokens': 40, 'total_tokens': 99}\n",
      "<class 'dict'> {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables = [\"country\"]    \n",
    ")\n",
    "prompt = country_detail_prompt.invoke({\"country\":\"Korea\"})\n",
    "print(type(prompt), prompt)\n",
    "# Json output 파서\n",
    "output_parser = JsonOutputParser()\n",
    "ai_message = llm.invoke(prompt)\n",
    "print(type(prompt), ai_message)\n",
    "json_result = output_parser.invoke(ai_message)\n",
    "print(type(json_result), json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b202202b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'Seoul',\n",
       " 'population': 51000000,\n",
       " 'language': 'Korean',\n",
       " 'currency': 'South Korean won'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables = [\"country\"]    \n",
    ")\n",
    "output_parser = JsonOutputParser()\n",
    "info = output_parser.invoke(llm.invoke(country_detail_prompt.invoke({'country':'korea'})))\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ac171",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "- Pydantic 모델을 사용하여 LLM 출력을 구조화된 형식으로 받기(JsonParser보다 훨신 안정적)\n",
    "- Pydantic : 데이터 유효성 검사, 설정관리를 간편하게 해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaaa2acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.User object at 0x0000023F6B150100>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class User:\n",
    "    def __init__(self, id, name, is_active=True):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "user = User(1, '홍길동')\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f0d7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='홍길동' is_active=True\n",
      "<class '__main__.User'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class User(BaseModel):\n",
    "    # gt=0 : id > 0 / ge=0 : id >= 0 / lt=0 : id < 0 / le=0 : id <= 0\n",
    "    id : int  = Field(gt=0,          description='id')\n",
    "    name:str  = Field(min_length=2,  description='name')\n",
    "    is_active:bool=Field(default=True, description='id활성화')\n",
    "user = User(id='1', name='홍길동')\n",
    "print(user)\n",
    "print(type(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5fbc177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables = [\"country\"]    \n",
    ")\n",
    "class CountryDetail(BaseModel):  # description : 더 적확한 출력 유도\n",
    "    capital:str     = Field(description='the capital of the country')\n",
    "    population:int  = Field(description='the population of the country')\n",
    "    language:str    = Field(description='the language of the country')\n",
    "    currency:str    = Field(description='the currency of the country')\n",
    "# 출력 형식 파서 + LLM\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "# output_parser = JsonOutputParser()  # 기존 방식\n",
    "# output_parser.invoke(llm.invoke(country_detail_prompt.invoke({'country':'korea'})))\n",
    "\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({'country':'korea'}))\n",
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91b97c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital='Seoul' population=51000000 language='Korean (Hangul)' currency='South Korean won'\n",
      "Seoul 51000000\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "print(info.capital, info.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff5d9997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 json :  {\"capital\":\"Seoul\",\"population\":51000000,\"language\":\"Korean (Hangul)\",\"currency\":\"South Korean won\"}\n",
      "info를 dict :  {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean (Hangul)', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print('info를 json : ', info.model_dump_json())  # json()\n",
    "print('info를 dict : ', info.model_dump())  # dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22b55e",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 렝체인 생성하기\n",
    "## 1) 문자열 출력 파서 사용\n",
    "- invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ae6fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model='llama3.2:1b', \n",
    "                 temperature=0  # 일관성 답변\n",
    "                )\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = 'What is the capital if {country}. Return the name of the city only',\n",
    "    input_variables = ['country']\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({'country':'korea'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfc774",
   "metadata": {},
   "source": [
    "## 2) LCEL을 사용한 간단한 체인 구성\n",
    "- 파이프연산자 (|) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f6eb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 템플릿 -> llm -> 출력 파서를 연결하는 체인 생성\n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "# 생성된 체인 invoke\n",
    "capital_chain.invoke({'country':'korea'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b09978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(capital_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed227ab",
   "metadata": {},
   "source": [
    "## 3) 복합 체인 구성\n",
    "- 여러단계의 추론이 필요한 경우(체인 연결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516c3326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라 설명 -> 나라명\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "country_prompt = PromptTemplate(\n",
    "    template = '''Guess the name of the country based on the following information:{information}\n",
    "    Return the name of the country only''',\n",
    "    input_variables=['information']\n",
    "    )\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({'information':'This country is very famous for its wine'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f396e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라명 추측 체인 생성\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "#type(country_chain)\n",
    "country_chain.invoke({'information':'This country is very famous for its wine'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나라 설명 (-> 나라명 ) -> 그 나라 수도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26eab03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = country_chain | capital_chain \n",
    "final_chain.invoke({'information':'This country is very famous for its wine'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79a7965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {'country':country_chain} | capital_chain\n",
    "final_chain.invoke({'information':'This country is very famous for its wine'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c82557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "final_chain = {'information':RunnablePassthrough()} | \\\n",
    "                {'country':country_chain} | capital_chain\n",
    "final_chain.invoke('This country is very famous for its wine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e126dc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 탬플릿에 변수가 2개\n",
    "# 나라 설명 -> 나라명\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "country_prompt = PromptTemplate(\n",
    "    template = '''Guess the name of the country in the {continent} based on the following information:{information}\n",
    "    Return the name of the country only''',\n",
    "    input_variables=['information','continent']\n",
    "    )\n",
    "# output_parser.invoke(llm.invoke(country_prompt.invoke({'information':'This country is very famous for its wine',\n",
    "#                                                       'continent':'Europe'})))\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "country_chain.invoke({'information':'This country is very famous for its wine',\n",
    "                      'continent':'Europe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "630b4fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {'country':country_chain} | capital_chain\n",
    "final_chain.invoke({'information':'This country is very famous for its wine',\n",
    "                      'continent':'Europe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ff25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6ef5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80faecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 나라명 (-> 제일 유명한 음식 ) -> 음식의 레시피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8714f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54fe912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df1bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a91e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883af1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef79e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2bd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c641e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
