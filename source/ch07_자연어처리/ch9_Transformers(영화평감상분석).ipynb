{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a0bdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:2px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:10pt;}\n",
       "div.text_cell_render.rendered_html{font-size:10pt;}\n",
       "div.output {font-size:10pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:10pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:11pt;padding:4px;}\n",
       "table.dataframe{font-size:10px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:2px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:10pt;}\n",
    "div.text_cell_render.rendered_html{font-size:10pt;}\n",
    "div.output {font-size:10pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:10pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:11pt;padding:4px;}\n",
    "table.dataframe{font-size:10px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbbb92",
   "metadata": {},
   "source": [
    "**<font size='6' color='red'>ch9. Transformers</font>**\n",
    "- 인코더 층만으로 구현(입력:자연어, 출력:긍정/부정)\n",
    "## 1. 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468a522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from time import time # 70.01.01 부터 현재까지의 밀리세컨\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c269c26",
   "metadata": {},
   "source": [
    "## 2. 하이퍼 파라미터 설정(이 파라미터를 바꾸면 정확도나 학습 속도에 차이남)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e3062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_WORDS = 10000  # imdb 데이터의 단어수\n",
    "MY_LENGTH = 80  # 영화평 단어수 80개만 독립변수\n",
    "MY_EMBED = 32   # embading layer의 결과 차원\n",
    "MY_HIDDEN = 64  # LSTM의 units 차원\n",
    "MY_EPOCH = 10  # 학습 수\n",
    "MY_BATCH = 200 # batch_size(fit시 매번 데이터를 가져오는 데이터)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b6b9e",
   "metadata": {},
   "source": [
    "## 3. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d81665",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MY_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cb24a",
   "metadata": {},
   "source": [
    "## 4. 문자단어 -> 정수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24eed22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "a\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "word_to_id = imdb.get_word_index()  # {'word':id}\n",
    "\n",
    "# 정수 -> 문자 단어\n",
    "id_to_word = {}  # {1:'the', 3:'a', 16816:'sonja'}\n",
    "for word, value in word_to_id.items():\n",
    "    id_to_word[value] = word\n",
    "print(id_to_word[1])\n",
    "print(id_to_word[3])\n",
    "print(id_to_word[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc04467",
   "metadata": {},
   "source": [
    "## 5.숫자 영화평 -> 자연어 영화평 return 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0061599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(review_num):\n",
    "    decoded = [id_to_word.get(num-3, '???') for num in review_num]\n",
    "    return ' '.join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5962edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal ??? the hair is big lots of boobs ??? men wear those cut ??? shirts that show off their ??? sickening that men actually wore them and the music is just ??? trash that plays over and over again in almost every scene there is trashy music boobs and ??? taking away bodies and the gym still doesn't close for ??? all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then 0\n"
     ]
    }
   ],
   "source": [
    "print(decoding(x_train[1]), y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b394f0",
   "metadata": {},
   "source": [
    "## 6. 영화평(입력변수)의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66dcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_length(x_train):\n",
    "    print('첫 20개 영화평 길이')\n",
    "    print([len(x_data) for x_data in x_train[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c0da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 20개 영화평 길이\n",
      "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130, 450, 99, 117, 238, 109, 129, 163, 752, 212, 177]\n"
     ]
    }
   ],
   "source": [
    "# pad_sequence\n",
    "show_length(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762f5bc",
   "metadata": {},
   "source": [
    "## 7. 모든 영화평 길이를 동일하게 (80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bb4b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 20개 영화평 길이\n",
      "[80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "첫 20개 영화평 길이\n",
      "[80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_sequences(x_train,\n",
    "                       padding='pre',\n",
    "                       truncating='pre',  # 뒷부분을 짜르고 앞 부분을 남김 / 'pre' : 앞 부분을 짜르고 뒷 부분을 남김\n",
    "                       maxlen=MY_LENGTH)\n",
    "X_test = pad_sequences(x_test,\n",
    "                      padding='pre',\n",
    "                      truncating='pre',\n",
    "                      maxlen=MY_LENGTH)\n",
    "show_length(X_train), show_length(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d8c97",
   "metadata": {},
   "source": [
    "## 8. 최종 데이터 shape확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4f73bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 80), (25000,), (25000, 80), (25000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5df54",
   "metadata": {},
   "source": [
    "## 9. 모델  생성(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6858e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 32)            320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 344,897\n",
      "Trainable params: 344,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MY_WORDS,  # 10000\n",
    "                   output_dim=MY_EMBED,  # 32\n",
    "                   input_length=MY_LENGTH,  # 80\n",
    "                   ))\n",
    "# RNN : 입력 단어의 길이 수가 너무 길면 파라미터 업데이트 안 됨\n",
    "# 개선모델 1. LSTM | 개선모델 2. GRU\n",
    "model.add(LSTM(units=MY_HIDDEN,\n",
    "              input_shape=(MY_LENGTH, MY_EMBED)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f21e5",
   "metadata": {},
   "source": [
    "## 9. 모델 생성(Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a355a74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 80, 32)       320000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 80, 32)      0           ['embedding[0][0]']              \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 80, 32)      12608       ['tf.__operators__.add[0][0]',   \n",
      " dAttention)                                                      'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 80, 32)       0           ['tf.__operators__.add[0][0]',   \n",
      "                                                                  'multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 80, 32)      128         ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 80, 32)       4192        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 80, 32)       0           ['sequential[0][0]',             \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 80, 32)      128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['batch_normalization_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 32)           0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           2112        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            130         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 339,298\n",
      "Trainable params: 339,170\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "INPUTS = layers.Input(shape=(MY_LENGTH,)) # 80\n",
    "\n",
    "INPUT_EMBEDDING = layers.Embedding(input_dim=MY_WORDS, # 10000\n",
    "                                    output_dim=MY_EMBED)(INPUTS)# 32\n",
    "# Positional Encoding\n",
    "POSITIONS = tf.range(start=0,\n",
    "                    limit=MY_LENGTH)\n",
    "POS_ENCODING = layers.Embedding(input_dim=MY_LENGTH, output_dim=MY_EMBED)(POSITIONS)\n",
    "POS_ENC_OUTPUT = POS_ENCODING + INPUT_EMBEDDING\n",
    "\n",
    "ATTENTION_OUTPUT = layers.MultiHeadAttention(num_heads=3,\n",
    "                                            key_dim=MY_EMBED)(POS_ENC_OUTPUT,\n",
    "                                            POS_ENC_OUTPUT)\n",
    "X = layers.add([POS_ENC_OUTPUT, ATTENTION_OUTPUT])\n",
    "X = layers.BatchNormalization()(X)\n",
    "\n",
    "# FeedForward Network\n",
    "FFN = Sequential([layers.Dense(MY_HIDDEN, activation=\"relu\"),\n",
    "                                layers.Dense(MY_EMBED, activation=\"relu\")])(X)\n",
    "X = layers.add([FFN, X])\n",
    "X = layers.BatchNormalization()(X)\n",
    "\n",
    "# 하나의 벡터로 압축해서 Dense로 보내줌\n",
    "X = layers.GlobalAveragePooling1D()(X)\n",
    "X = layers.Dropout(0.1)(X)\n",
    "\n",
    "X = layers.Dense(MY_HIDDEN, activation=\"relu\")(X)\n",
    "X = layers.Dropout(0.1)(X)\n",
    "\n",
    "OUTPUTS = layers.Dense(2, activation=\"softmax\")(X)\n",
    "model = Model(inputs=INPUTS, outputs=OUTPUTS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451e764",
   "metadata": {},
   "source": [
    "## 10. 학습환경 설정 및 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9026b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750733893.0996554\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 16s 152ms/step - loss: 0.4783 - acc: 0.7658 - val_loss: 0.6152 - val_acc: 0.6282\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 0.2802 - acc: 0.8863 - val_loss: 0.5853 - val_acc: 0.6392\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 0.2195 - acc: 0.9127 - val_loss: 0.5284 - val_acc: 0.7288\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 0.1791 - acc: 0.9301 - val_loss: 0.4315 - val_acc: 0.8012\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 15s 151ms/step - loss: 0.1410 - acc: 0.9433 - val_loss: 0.6053 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 15s 150ms/step - loss: 0.1202 - acc: 0.9484 - val_loss: 0.6223 - val_acc: 0.8058\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.1009 - acc: 0.9562 - val_loss: 0.9235 - val_acc: 0.7952\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 0.0808 - acc: 0.9658 - val_loss: 1.3841 - val_acc: 0.7692\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0714 - acc: 0.9703 - val_loss: 1.1116 - val_acc: 0.8010\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0526 - acc: 0.9790 - val_loss: 1.6463 - val_acc: 0.8062\n",
      "총 학습 시간 :  156.19479894638062\n"
     ]
    }
   ],
   "source": [
    "model.compile(#loss='binary_crossentropy',  # 이진분류시 손실함수\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "\n",
    "begin = time() # 70.1.1부터 현재까지의 세컨\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                epochs=MY_EPOCH,\n",
    "                batch_size=MY_BATCH,\n",
    "                validation_split=0.2,\n",
    "                verbose=1)\n",
    "end = time()\n",
    "print('총 학습 시간 : ', (end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab7f27c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhist\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7beaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, loss_ax = plt.subplots(figsize=(12,6))\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "acc_ax = loss_ax.twinx()\n",
    "acc_ax.plot(hist.history['acc'], 'g', label='train accuracy')\n",
    "acc_ax.plot(hist.history['val_acc'], 'b', label='val accuracy')\n",
    "loss_ax.set_xlabel('epochs')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "loss_ax.legend(loc='center right')\n",
    "acc_ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297b45c",
   "metadata": {},
   "source": [
    "## 11. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49138c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 11ms/step - loss: 1.6444 - acc: 0.7962\n",
      "정확도 :  0.7961999773979187\n"
     ]
    }
   ],
   "source": [
    "# pad_sequences 'post' : 75.6%\n",
    "# pad_sequences 'pre' : 80.1%\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print('정확도 : ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e91c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬, recall, precision을 위한 yhat\n",
    "yhat = (model.predict(X_test, verbose=0) > 0.5).astype(np.int16).reshape(-1)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬\n",
    "confusion_matrix(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall ( 실제 True인 것 중 True로 예측한 비율) 9853 /  ( 2647+9853)\n",
    "recall_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision(True로 예측한 것 중 실제값이 True인 비율) 9853 / (2307+9853)\n",
    "precision_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b4d56",
   "metadata": {},
   "source": [
    "## 12. 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5009933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 16, 2070, 31, 7, 148, 1281, 108, 121, 13, 340, 416, 1406, 7, 58, 11, 4, 750, 2721, 13, 873, 12, 8, 794, 6, 727, 769, 21, 39, 4, '???', '???', 4, 114, 828, '???', 61, 1398, 1895, 72, 1562, 3953, 366, 4, 55, 130, 4, 354, 34, 4, 485, 156, 71, 572, 1159, 36, 1319, 8, 2833, 60, 4, 91, 1302, 921, 6902, 19, 1739, '???', 4, 627, 5, 816, 82, '???', 4, 65, 950, 8703, '???', 4, 444, '???', 585, 51, 13, 2525, 91, 16, 89, 4, 22, '???', 11, 61, 2334, 196, 103, 1200, 4, 750, 12, '???', 958, 7, 3958, 19, 369, 5, 13, 258, 546, 1786, 8, 106, 12, 174, 198, 89, 2349, 4, 585, 16, 45, 4, 243, 7, 20, 15, 495, 199, 17, 641, 722, 5, 17, 142, 15, 408, 25, 958, 8, 104, 44, 474, 407, 386, 12, 8, 6, 1876, 311] 152\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"This was genuinely one of those rare films where I completely lost track of time in the theater. \n",
    "Initially, I expected it to follow a predictable storyline, \n",
    "but from the midpoint onward, the plot kept subverting my expectations, keeping me thoroughly engaged until the very end.\n",
    "The performances by the lead actors were particularly impressive. \n",
    "They managed to convey even the most subtle emotional nuances with remarkable naturalness. \n",
    "The cinematography and soundtrack also complemented the story perfectly, significantly enhancing the overall immersive experience.\n",
    "What I appreciated most was how the film lingered in my thoughts long after leaving the theater. \n",
    "It sparked plenty of conversations with friends, and I found myself wanting to watch it again – that's how satisfying the experience was.\n",
    "It's the kind of movie that works both as light entertainment and as something that gives you plenty to think about. \n",
    "I'd definitely recommend it to a wide audience!~!@#$%^&\n",
    "\"\"\".lower()\n",
    "import re\n",
    "review = re.sub('[^a-zA-Z\\'\\s]', '', review)\n",
    "#print('영화평(특수문자 제외) : ', review)\n",
    "review = review.split()  # 단어 list\n",
    "review = [1] + [word_to_id.get(word, -1)+3 if word_to_id.get(word, -1) < 10000 else '???' for word in review]\n",
    "print(review, len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e3b22c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '???'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtruncating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMY_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m input_data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\keras\\utils\\data_utils.py:1078\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[1;32m-> 1078\u001b[0m trunc \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '???'"
     ]
    }
   ],
   "source": [
    "input_data = pad_sequences([review],\n",
    "                      padding='pre',\n",
    "                      truncating='pre',\n",
    "                      maxlen=MY_LENGTH)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1874c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = (model.predict(input_data) > 0.5).astype('int8').reshape(-1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0d9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf7b7fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 20, 16, 66, 1127, 5, 13, 296, 12, 469, 4, 20, 1383, 12, 586, 130, 209, 149, 4, 5431, 13, 2303, 386, 12, 12, 16, 60, 53, 221, 536, 15, 12, 100, 593, 11, 147, 113, 4, 293, 109, 16, 2252, 5, 917, 73, 38, 61, 523, 71, 654, 5, 4, 65, 16, 253] 56\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"The movie was really exciting, \n",
    "and I watched it throughout the movie hoping it wouldn't end without \n",
    "watching the clock. I strongly recommend it. \n",
    "It was even more interesting thinking that it could happen in real life. \n",
    "The main character was handsome and acted well, so my eyes were happy, \n",
    "and the story was fun @_@ㅠ.ㅠ\"\"\".lower()\n",
    "import re\n",
    "review = re.sub('[^a-zA-Z\\'\\s]', '', review)\n",
    "review = review.split() # 단어 list\n",
    "review = [1] + [word_to_id.get(word, -1)+3 for word in review]\n",
    "print(review, len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa7078d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    1,    4,   20,   16,   66, 1127,    5,   13,  296,\n",
       "          12,  469,    4,   20, 1383,   12,  586,  130,  209,  149,    4,\n",
       "        5431,   13, 2303,  386,   12,   12,   16,   60,   53,  221,  536,\n",
       "          15,   12,  100,  593,   11,  147,  113,    4,  293,  109,   16,\n",
       "        2252,    5,  917,   73,   38,   61,  523,   71,  654,    5,    4,\n",
       "          65,   16,  253]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pad_sequences([review],\n",
    "                      padding='pre',\n",
    "                      truncating='pre',\n",
    "                      maxlen=MY_LENGTH)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7adf4d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = (model.predict(input_data)>0.5).astype('int8').reshape(-1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134acfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64c1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71be40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcd84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063a9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
